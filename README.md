# CNN-MoE-Interpretability

This repository contains code for reproducing experiments and analysis from the paper:

**"Sparsely-gated Mixture-of-Expert Layers for CNN Interpretability"**  
by Svetlana Pavlitska et al.

## ðŸŽ¯ Project Goals

The project focuses on reproducing the following results from the paper:

- **Figure 3**: t-SNE visualizations of gate logits for a ResBlock-MoE with 4 experts and a GAP-FC gate.
- **Figure 4**: t-SNE visualization of sample assignments to experts.
- **Table III**: Class-wise expert weight allocation showing which classes each expert specializes in.

## ðŸ—‚ Repository Structure

```bash
.
â”œâ”€â”€ models/
â”‚   â””â”€â”€ moe_layer/resnet18/          # ResBlock-MoE architecture
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ cifar100/                    # Training scripts for CIFAR-100
â”œâ”€â”€ analysis/resnet18/
â”‚   â”œâ”€â”€ tsne_visualization.py       # Reproduces Fig. 3 and Fig. 4
â”‚   â”œâ”€â”€ expert_utilization.py       # Reproduces Table III
â”‚   â””â”€â”€ per_expert_accuracy.py      # Optional: Accuracy per expert
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ cifar100_dataset.py         # CIFAR-100 loading logic
â”œâ”€â”€ requirements.txt                # Dependencies
